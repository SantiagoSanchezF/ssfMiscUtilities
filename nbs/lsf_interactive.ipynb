{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c334a3e0-ec5a-4fa0-902f-14363b3c055d",
   "metadata": {},
   "source": [
    "# LSF interactive\n",
    "\n",
    "> send LSF jobs and colect results in interactive sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fd1073-2be6-4c07-9c69-3364ae18a966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp lsf_interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6d11eb-e0e9-4a6f-94bd-08ab16bd5d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878226e-3d9a-40ff-b66c-25104440a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "import multiprocessing\n",
    "from collections import Counter\n",
    "\n",
    "from fastcore.script import call_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9967142-fe83-4845-a841-1c6695ae0357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest \n",
    "from ssfMiscUtilities.generic import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44d9eb4-0a4a-41d1-ac91-0e24c6792173",
   "metadata": {},
   "source": [
    "## Generic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e9eac4-ee7d-4ffa-aa12-575007849278",
   "metadata": {},
   "source": [
    "## Individual job handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aead1af8-1265-4024-b62c-7886fac7e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class lsf_job:\n",
    "    def __init__(\n",
    "        self,\n",
    "        bsub_args,\n",
    "        job_name,\n",
    "        output_file,\n",
    "        args,\n",
    "        job_id=None,\n",
    "        status=\"unsend\",\n",
    "        iteration=1,\n",
    "        iteration_lim=3,\n",
    "        exit_code=None,\n",
    "        read_func=None\n",
    "    ):\n",
    "        self.bsub_args = bsub_args\n",
    "        self.job_name = job_name\n",
    "        self.output_file = output_file\n",
    "        self.args = args\n",
    "        self.job_id = job_id\n",
    "        self.stat = status\n",
    "        self.iteration = iteration\n",
    "        self.iter_lim = iteration_lim\n",
    "        self.result = None\n",
    "        self.stderr = None\n",
    "        self.stdout = None\n",
    "        self.exit_code = exit_code\n",
    "        self.read_func = read_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec1cbbe-025e-4de3-9f79-6ecde1e1a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def submit(tup):\n",
    "    bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func = tup\n",
    "    cmd = \"bsub -q {} -M {} -n {} -J {} -o {} -e {} {} {} {}\".format(\n",
    "        bsub_args[\"queue\"],\n",
    "        int(bsub_args[\"mem\"] + (bsub_args[\"mem\"] * (iteration - 1))),\n",
    "        bsub_args[\"n_cpus\"],\n",
    "        job_name,\n",
    "        f'{bsub_args[\"output_dir\"]}/{job_name}.stdout',\n",
    "        f'{bsub_args[\"output_dir\"]}/{job_name}.stderr',\n",
    "        bsub_args[\"interpreter\"],\n",
    "        bsub_args[\"script\"],\n",
    "        \" \".join(map(str, args)),\n",
    "    )\n",
    "    process = subprocess.Popen(\n",
    "        cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
    "    )\n",
    "    output, err = process.communicate()\n",
    "    \n",
    "    if err != b\"\":\n",
    "        # raise ValueError(\"error in sending job\")\n",
    "        stat = \"unsend\"\n",
    "    else:\n",
    "        job_id = re.split(\"[<>]\", output.decode())[1]\n",
    "        exit_code = None\n",
    "        stat = f\"send it:{iteration}\"\n",
    "    return bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8c1e27-4b61-49bb-8494-439b74cf755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def update_status(tup):\n",
    "    bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func = tup\n",
    "    if stat == \"unsend\":\n",
    "        bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func = submit((bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func))\n",
    "        return bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func\n",
    "    if stat == \"done\":\n",
    "        return None\n",
    "    cmd = f\"bjobs {job_id}\"\n",
    "    process = subprocess.Popen(\n",
    "        cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
    "    )\n",
    "    output, err = process.communicate()\n",
    "    stat = (\n",
    "        dict(list(zip(*[x.split() for x in output.decode().split(\"\\n\")][:2])))\n",
    "        .get(\"STAT\",\"EXIT\")[:4]\n",
    "        .lower()\n",
    "    )\n",
    "    if stat in [None, \"exit\"]:\n",
    "        stdout_file = f'{bsub_args[\"output_dir\"]}/{job_name}.stdout'\n",
    "        if os.path.isfile(stdout_file) == False:\n",
    "            stat = False\n",
    "            return\n",
    "        with open(stdout_file) as h:\n",
    "            rev = list(reversed(list(h)))\n",
    "            for ix, x in enumerate(rev):\n",
    "                if \"Sender\" in x:\n",
    "                    stat = rev[ix - 1].split()[-1][:4].lower()\n",
    "                    break\n",
    "    if stat == \"done\":\n",
    "        bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func = get_result((bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func))\n",
    "    if stat == \"exit\":\n",
    "        for ix, x in enumerate(rev):\n",
    "            if \"Exited with exit code\" in x:\n",
    "                exit_code = int(float(x.split()[-1]))\n",
    "                if (\n",
    "                    \"TERM_MEMLIMIT\" in rev[ix + 1]\n",
    "                ):  # exit_code == 1 and iter < iter_lim:\n",
    "                    iteration += 1\n",
    "                    bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func = submit((bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func))\n",
    "                    # stat = None\n",
    "                break\n",
    "    if exit_code not in [None]:\n",
    "        stderr_file = f'{bsub_args[\"output_dir\"]}/{job_name}.stderr'\n",
    "        if os.path.isfile(stderr_file) == False:\n",
    "            stderr = False\n",
    "            return\n",
    "        with open(stderr_file) as h:\n",
    "            stderr = list(h)\n",
    "    return bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d18120a-5403-44e5-85c7-9060b54d4090",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_result(tup):\n",
    "    bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func = tup\n",
    "    result = None\n",
    "    if read_func==None:\n",
    "        pickle_file = output_file\n",
    "        if os.path.isfile(pickle_file) == False:\n",
    "            stat = False\n",
    "            return output_file,stat,result\n",
    "        with open(pickle_file, \"rb\") as h:\n",
    "            result = pickle.load(h)\n",
    "    else:\n",
    "        result = read_func(output_file)\n",
    "    return bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc0d358-08f9-4b34-8fd5-9ef612a1852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LSF:\n",
    "    \" send a series of lsf jobs, collect the objects, and clean. Output objects must be a pickle \"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        script,\n",
    "        jobs,\n",
    "        interpreter=\"python3\",\n",
    "        monitor_file=\"/homes/fragoso/generic_monitor_file.txt\",\n",
    "        output_dir=\"/hps/nobackup/rdf/metagenomics/research-team/santiago/.std.generic\",\n",
    "        mem=1000,\n",
    "        n_cpus=1,\n",
    "        queue=\"research\",\n",
    "        n_jobs =1,\n",
    "    ):\n",
    "        self.monitor_file = monitor_file\n",
    "        self.bsub_args = {\n",
    "            \"script\": script,\n",
    "            \"interpreter\": interpreter,\n",
    "            \"output_dir\": output_dir,\n",
    "            \"mem\": mem,\n",
    "            \"n_cpus\": n_cpus,\n",
    "            \"queue\": queue,\n",
    "        }\n",
    "        self.n_jobs = n_jobs\n",
    "        self.jobs_ = jobs\n",
    "        self.jobs = {job_name:lsf_job(self.bsub_args, job_name, output_file, args)\n",
    "            for job_name, output_file, args in self.jobs_\n",
    "                    }\n",
    "        try:\n",
    "            os.mkdir(output_dir)\n",
    "        except FileExistsError as err:\n",
    "            print(err)\n",
    "            print(\"Force gather mode\")\n",
    "            for _,j in self.jobs.items():\n",
    "                j.stat=None\n",
    "\n",
    "    def update(self,read_func=None,loop=False,verbose=1):\n",
    "        summary_ = {'start'}\n",
    "        it = 0\n",
    "        while set(summary_)!={'done'}:\n",
    "            chunk = [(j.bsub_args, j.job_name, j.output_file, j.args, j.job_id, j.stat, j.iteration, j.iter_lim, j.result, j.stderr, j.stdout, j.exit_code, j.read_func if read_func==None else read_func )\n",
    "                        for k,j in self.jobs.items() if j.stat!=\"done\"]\n",
    "            if verbose>1:\n",
    "                print(f\"processing {len(chunk)} jobs\")\n",
    "            step = 0\n",
    "            size = 500\n",
    "            while step*size<len(chunk):\n",
    "                with multiprocessing.Pool(self.n_jobs) as pool:\n",
    "                    res = pool.map(update_status,chunk[step*size:(step+1)*size])\n",
    "                for bsub_args, job_name, output_file, args, job_id, stat, iteration, iter_lim, result, stderr, stdout, exit_code, read_func in res:\n",
    "                    self.jobs[job_name].bsub_args = bsub_args\n",
    "                    self.jobs[job_name].job_name = job_name\n",
    "                    self.jobs[job_name].output_file = output_file\n",
    "                    self.jobs[job_name].args = args\n",
    "                    self.jobs[job_name].job_id = job_id\n",
    "                    self.jobs[job_name].stat = stat\n",
    "                    self.jobs[job_name].iteration = iteration\n",
    "                    self.jobs[job_name].iter_lim = iter_lim\n",
    "                    self.jobs[job_name].result = result\n",
    "                    self.jobs[job_name].stderr = stderr\n",
    "                    self.jobs[job_name].stdout = stdout\n",
    "                    self.jobs[job_name].exit_code = exit_code\n",
    "                    self.jobs[job_name].read_func = read_func\n",
    "                if verbose>1:    \n",
    "                    print(f\"step {step} done\")\n",
    "                step += 1\n",
    "                del res\n",
    "\n",
    "            summary = dict(Counter([job.stat for _,job in self.jobs.items()]))\n",
    "            summary_ = dict(summary)\n",
    "            if verbose>1:\n",
    "                print(summary)\n",
    "            if loop==False:\n",
    "                sumary_= {'done'}\n",
    "            else:\n",
    "                if verbose>0:\n",
    "                    # print(f\"\",end='\\r',flush=True)\n",
    "                    print(f\"iter: {it}\\t{str(summary)}\\t\\t\\t\",end='\\r',flush=True)\n",
    "                time.sleep(20)\n",
    "                it += 1\n",
    "        return summary\n",
    "\n",
    "    def clean(self):\n",
    "        shutil.rmtree(self.bsub_args[\"output_dir\"])\n",
    "\n",
    "    def retake(self, code):\n",
    "        \" keep gathering results after the original kernel is dead \"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ff7318-02b0-478a-8926-91aa4e9c5a39",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41d2eb5-19c6-4966-9f24-82cea5ade2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /homes/fragoso/Downloads/test.py\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "%%writefile /homes/fragoso/Downloads/test.py\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# time.sleep(50)\n",
    "outf, v = sys.argv[1:]\n",
    "\n",
    "a = {\"hello\": v}\n",
    "\n",
    "with open(outf, \"wb\") as handle:\n",
    "    pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408d9b76-7cd4-4833-adf4-65dcd981ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "iitt=101110033822"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90a6447-0fca-41e8-9c4a-42452015713c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: '/homes/fragoso/Downloads/this101110033822'\n",
      "Force gather mode\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "\" this is the jobs array :: format.. job_name, output_file, args \"\n",
    "outputd = f'/homes/fragoso/Downloads/this{iitt}'\n",
    "# iitt+=1\n",
    "jobs_arr = [\n",
    "    (\n",
    "        str(x),\n",
    "        f'{outputd}/{x}.pickle',\n",
    "        [f'{outputd}/{x}.pickle', x],\n",
    "    )\n",
    "    for x in range(10)\n",
    "]\n",
    "\n",
    "jobs = LSF(\"/homes/fragoso/Downloads/test.py\",jobs_arr,n_jobs=4,output_dir=outputd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a20e04-209d-40dc-8dd4-ba149d7b1340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 10 jobs\n",
      "step 0 done\n",
      "{'done': 10}\n",
      "iter: 0\t{'done': 10}\t\t\t\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'done': 10}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "def foo(file):\n",
    "    return \"dsdsd\"\n",
    "jobs.update(read_func=foo,loop=True,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3abed4-d974-440c-b218-c083864665ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f658400e-ca11-40aa-b0bb-768c7c7f1b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
